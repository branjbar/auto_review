"""
Here we write a python code which gets a collection of Google Scholar html pages and explores the intersting patterns.
"""
import pickle
import random
from urllib import FancyURLopener
import webbrowser
from bs4 import BeautifulSoup
import re
import operator
import time
import glob
import networkx  



MAIN_FOLDER = "/HTML_FILES/"  # where you store all your downloaded Google pages.


class MyOpener(FancyURLopener):
    version = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko)'
open_url = MyOpener().open


network = networkx.DiGraph()
paper_dict = {}


def read_citation_html(address):
    """ (string) --> (addition of nodes and edges to the network)
    reads an html papge provided as "address" and extracts the papers mentioned in it and adds the information to a network
    """
    soup = BeautifulSoup(open_url(address).read())

    # get unique_key and the title of main paper on top of this page
    unique_key = soup.find("input", {"name": "cites"}).get('value', '0')
    title = soup.find("div", {"id": "gs_rt_hdr"}).a.text

    # check if the paper is already in the database or not
    if unique_key not in paper_dict.keys():

        # define a new paper
        main_paper = Paper(unique_key, title)
        paper_dict[unique_key] = main_paper
        network.add_node(main_paper.unique_key, main_paper.get_graph_attr())

    else:

        # get the paper from database
        main_paper = paper_dict[unique_key]

    # get list of papers mentioned in the page
    for cite_paper in soup.findAll("div", {"class": "gs_r"}):

        title = cite_paper.find("h3", {"class": "gs_rt"}).text.replace('[HTML]', '').replace('[BOOK][B]', '').strip()
        authors = cite_paper.find("div", {"class": "gs_a"}).text.strip().encode('ascii', 'ignore')

        try:
            abstract = cite_paper.find("div", {"class": "gs_rs"}).text.replace('Abstract ', '').strip()
        except:

            # the paper hasn't had any abstract!!
            abstract = ''
            pass

        u_key = 0
        for a in cite_paper.findAll("div", {"class": "gs_fl"}):
            if "cites=" in a.find("a").get("href"):
                href = str(a.find("a").get("href"))
                num_citation = a.find("a").text.split(' ')[-1]
                u_key = re.search('cites=(.*)&as', href).group(1)
                break

        if u_key:
            if u_key not in paper_dict.keys():
                paper = Paper(u_key, title, authors, abstract, num_citation)
                paper_dict[u_key] = paper
            else:

                # get the paper from paper_dict and update its attributes
                paper = paper_dict[u_key]
                paper.title = title.title()
                paper.authors = authors
                paper.abstract = abstract.replace('\\bf', '').replace('_', '-').replace('\d', '\ d').replace('^', '')
                paper.num_citation = num_citation
                paper_dict[u_key] = paper

            network.add_node(paper.unique_key, paper.get_graph_attr())
            network.add_edge(main_paper.unique_key, paper.unique_key)
            main_paper.cites.append(paper.unique_key)


class Paper():
    """
    a class to store the information of each paper suh as its unique key, title, etc.
    """

    def __init__(self, unique_key, title='', authors='', abstract='', num_citation=0):
        self.unique_key = unique_key
        self.title = title.title()
        self.abstract = abstract.replace('\\bf', '').replace('_', '-').replace('\d', '\ d').replace('^', '')
        self.authors = authors
        self.num_citation = num_citation
        self.cites = []
        self.citation_url = "http://scholar.google.nl/scholar?cites=" + self.unique_key + "&hl=en"

    def get_graph_attr(self):
        return {'title': self.title,
                'authors': self.authors,
                'citations': self.num_citation,
                'unique_key': self.unique_key}


class Latex():
    """
    a class to generate a LaTeX file from the constructed network.
    """
    
    def __init__(self):
        self.text = """
\documentclass{article}
\usepackage[a4paper, margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[final]{pdfpages}


\\title{Report 3: \\\ Auto Generated Review Paper}
\\author{Bijan Ranjbar-Sahraei}
%\date{November 2014}

\\begin{document}

\maketitle



"""
        self.text += "\\abstract{In this report, we analyse the most important papers on the topic of (complex) networks and introduce their important sub-topics. This report is generated by analysis of %d papers and the %d citation links among them.} \n" % (
            network.number_of_nodes(), network.number_of_edges())

    def add_section(self, section_name):
        self.text += "\section{%s} \n\n" % section_name

    def add_sub_section(self, sub_section_name):
        self.text += "\subsection{%s} \n\n" % sub_section_name


    def add_paper(self, paper):
        self.text += "\paragraph{%s} \n" % paper.title
        self.text += "\\textit{%s} \n\n" % paper.authors
        self.text += " %s\n \n \n" % paper.abstract


    def __finish__(self):
        self.text += """
\\begin{figure}
\includegraphics[height=.7\paperheight]{network}
\caption{Citation Network. For better visualization, the low degree nodes are removed from this figure. The size of the nodes increases by their degree. Each color refers to a specific topic.}
\end{figure}

"""
        self.text += "\end{document}"

        self.text = self.text.replace('&', '\&').replace('#', '').replace('$', '')


def write_latex():
    """
    discovers the important papers and adds them to the LaTeX file.
    """
    
    latex = Latex()

#    latex.add_section("The Most Generic Papers (High in-Degree)")
#    for unique_key in statistics['in_degree']:
#        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("The Most Popular Papers (High Pagerank)")
    for unique_key in statistics['pagerank']:
        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("The Most Influential Papers (High Hits)")
    for unique_key in statistics['hits']:
        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("The Papers which Bridge Different (High Betweenness)")
    for unique_key in statistics['betweenness']:
        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("Paper Topics")
    for index, paper_set in enumerate(statistics['modules']):
        latex.add_sub_section("Topic %d" % (index + 1))
        for unique_key in paper_set:
            latex.add_paper(paper_dict[str(unique_key)])

    latex.__finish__()

    with open(MAIN_FOLDER + "latex_report/latex.tex", "wa") as f:
        f.write(latex.text.encode('ascii', 'ignore'))


def read_html_files():
    """
    parses through all the html file and uses the read_citation_html module to add their informaiton to the network.
    """
    for index, file in enumerate(glob.glob(MAIN_FOLDER + "archive/*.html")):
        print 'File %d imported!' % index, file
        address = "file://" + file.replace(' ', '%20')
        read_citation_html(address)

    networkx.write_gml(network, MAIN_FOLDER + "paper_network.gml")
    with open(MAIN_FOLDER + "network.pkl", "wa") as f:
        pickle.dump(network, f)
    with open(MAIN_FOLDER + "paper_dict.pkl", "wa") as f:
        pickle.dump(paper_dict, f)



def get_statistics():
    """
    uses data statistics to find the most important papers in the collection.
    """
    
    
    with open(MAIN_FOLDER + "network.pkl", "r") as f:
        network = pickle.load(f)

    betweenness_dict = networkx.betweenness_centrality(network.to_undirected())
    sorted_betweenness = sorted(betweenness_dict.items(), key=operator.itemgetter(1), reverse=True)
    betweenness = [x[0] for x in sorted_betweenness[:5]]

    pagerank_dict = networkx.pagerank(network.to_undirected())
    sorted_pagerank = sorted(pagerank_dict.items(), key=operator.itemgetter(1), reverse=True)
    pagerank = [x[0] for x in sorted_pagerank[:5]]

    hits_dict = networkx.hits(network.to_undirected())
    sorted_hits = sorted(hits_dict[0].items(), key=operator.itemgetter(1), reverse=True)
    hits = [x[0] for x in sorted_hits[:5]]

    in_degree_dict = network.in_degree()
    sorted_in_degree = sorted(in_degree_dict.items(), key=operator.itemgetter(1), reverse=True)
    in_degree = [x[0] for x in sorted_in_degree[:5]]

    community_dict = []
    for k in xrange(20):
        community_dict += list(networkx.k_clique_communities(network.to_undirected(), 21 - k))
        if community_dict:
            break

    modules = []
    for index, community in enumerate(community_dict):
        modules.append([])
        for p in community:
            modules[index].append(p)

    statistics = {'in_degree': in_degree, 'betweenness': betweenness, 'hits': hits, 'pagerank': pagerank,
                  'modules': modules}

    with open(MAIN_FOLDER + "statistics.pkl", "wa") as f:
        pickle.dump(statistics, f)


def auto_explore():
    """
    as we have to manually save the useful pages, here we open the random pages in the browser in order to grow our network.
    """
    tmp_list = []
    for paper in paper_dict.values():
        tmp_list.append(paper)

    random.shuffle(tmp_list)

    for paper in tmp_list:
        if not paper.cites:
            webbrowser.open_new_tab(paper.citation_url)
            time.sleep(3)


if __name__ == "__main__":
    
    # Load the existing paper database
    with open(MAIN_FOLDER + "paper_dict.pkl", "r") as f:
        paper_dict = pickle.load(f)
    
    # load the existing network database
    with open(MAIN_FOLDER + "network.pkl", "r") as f:
        network = pickle.load(f)
    
    # use the statistics to find the important papers
    with open(MAIN_FOLDER + "statistics.pkl", "r") as f:
        statistics = pickle.load(f)

    # add more papers by reading more files
    read_html_files()
    
    # get statistics again
    get_statistics()
    
    # export the LaTeX survey
    write_latex()
