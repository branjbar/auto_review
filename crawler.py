"""
Here we write a python class called "Crawler" which gets a google scholar link and harvest all the related data on that page!
"""
import pickle
import random
from urllib import FancyURLopener
import webbrowser
from bs4 import BeautifulSoup
import re
import operator
import time
import glob



MAIN_FOLDER = "/Users/bijan/Dropbox/#PAPERS/#pyScholar/behavioral_robotics/"
# MAIN_FOLDER = "/Users/bijan/Dropbox/#PAPERS/#pyScholar/complex_networks/"


class MyOpener(FancyURLopener):
    version = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko)'


open_url = MyOpener().open

# in_degree = [7753900338122914721, 3742137919073350645, 11311448141672280414]  # inspired by most other papers.
# betweenness = [8469961665679949106, 3813657992288142888, 18128359152601397349, 7605262999426902927,
# 11663521936672698608]
# hub = [7753900338122914721, 3742137919073350645, 11311448141672280414, 3813657992288142888, 2346808897114984913,
# 13880079318989842329, 14362380329320495967, 2281355390010472193]
# pagerank = [11311448141672280414]

import networkx

network = networkx.DiGraph()
paper_dict = {}


def read_citation_html(address):
    soup = BeautifulSoup(open_url(address).read())

    # get unique_key and the title of main paper on top of this page
    unique_key = soup.find("input", {"name": "cites"}).get('value', '0')
    title = soup.find("div", {"id": "gs_rt_hdr"}).a.text

    # check if the paper is already in the database or not
    if unique_key not in paper_dict.keys():

        # define a new paper
        main_paper = Paper(unique_key, title)
        paper_dict[unique_key] = main_paper
        network.add_node(main_paper.unique_key, main_paper.get_graph_attr())

    else:

        # get the paper from database
        main_paper = paper_dict[unique_key]

    # get list of papers mentioned in the page
    for cite_paper in soup.findAll("div", {"class": "gs_r"}):

        title = cite_paper.find("h3", {"class": "gs_rt"}).text.replace('[HTML]', '').replace('[BOOK][B]', '').strip()
        authors = cite_paper.find("div", {"class": "gs_a"}).text.strip().encode('ascii', 'ignore')

        try:
            abstract = cite_paper.find("div", {"class": "gs_rs"}).text.replace('Abstract ', '').strip()
        except:

            # the paper hasn't had any abstract!!
            abstract = ''
            pass

        u_key = 0
        for a in cite_paper.findAll("div", {"class": "gs_fl"}):
            if "cites=" in a.find("a").get("href"):
                href = str(a.find("a").get("href"))
                num_citation = a.find("a").text.split(' ')[-1]
                u_key = re.search('cites=(.*)&as', href).group(1)
                break

        if u_key:
            if u_key not in paper_dict.keys():
                paper = Paper(u_key, title, authors, abstract, num_citation)
                paper_dict[u_key] = paper
            else:

                # get the paper from paper_dict and update its attributes
                paper = paper_dict[u_key]
                paper.title = title.title()
                paper.authors = authors
                paper.abstract = abstract.replace('\\bf', '').replace('_', '-').replace('\d', '\ d').replace('^', '')
                paper.num_citation = num_citation
                paper_dict[u_key] = paper

            network.add_node(paper.unique_key, paper.get_graph_attr())
            network.add_edge(main_paper.unique_key, paper.unique_key)
            main_paper.cites.append(paper.unique_key)


class Paper():
    def __init__(self, unique_key, title='', authors='', abstract='', num_citation=0):
        self.unique_key = unique_key
        self.title = title.title()
        self.abstract = abstract.replace('\\bf', '').replace('_', '-').replace('\d', '\ d').replace('^', '')
        self.authors = authors
        self.num_citation = num_citation
        self.cites = []
        self.citation_url = "http://scholar.google.nl/scholar?cites=" + self.unique_key + "&hl=en"

    def get_graph_attr(self):
        return {'title': self.title,
                'authors': self.authors,
                'citations': self.num_citation,
                'unique_key': self.unique_key}


class Latex():
    def __init__(self):
        self.text = """
\documentclass{article}
\usepackage[a4paper, margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[final]{pdfpages}


\\title{Report 3: \\\ Auto Generated Review Paper}
\\author{Bijan Ranjbar-Sahraei}
%\date{November 2014}

\\begin{document}

\maketitle



"""
        self.text += "\\abstract{In this report, we analyse the most important papers on the topic of (complex) networks and introduce their important sub-topics. This report is generated by analysis of %d papers and the %d citation links among them.} \n" % (
            network.number_of_nodes(), network.number_of_edges())

    def add_section(self, section_name):
        self.text += "\section{%s} \n\n" % section_name

    def add_sub_section(self, sub_section_name):
        self.text += "\subsection{%s} \n\n" % sub_section_name


    def add_paper(self, paper):
        self.text += "\paragraph{%s} \n" % paper.title
        self.text += "\\textit{%s} \n\n" % paper.authors
        self.text += " %s\n \n \n" % paper.abstract


    def __finish__(self):
        self.text += """
\\begin{figure}
\includegraphics[height=.7\paperheight]{network}
\caption{Citation Network. For better visualization, the low degree nodes are removed from this figure. The size of the nodes increases by their degree. Each color refers to a specific topic.}
\end{figure}

"""
        self.text += "\end{document}"

        self.text = self.text.replace('&', '\&').replace('#', '').replace('$', '')


def write_latex():
    latex = Latex()

#    latex.add_section("The Most Generic Papers (High in-Degree)")
#    for unique_key in statistics['in_degree']:
#        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("The Most Popular Papers (High Pagerank)")
    for unique_key in statistics['pagerank']:
        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("The Most Influential Papers (High Hits)")
    for unique_key in statistics['hits']:
        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("The Papers which Bridge Different (High Betweenness)")
    for unique_key in statistics['betweenness']:
        latex.add_paper(paper_dict[str(unique_key)])

    latex.add_section("Paper Topics")
    for index, paper_set in enumerate(statistics['modules']):
        latex.add_sub_section("Topic %d" % (index + 1))
        for unique_key in paper_set:
            latex.add_paper(paper_dict[str(unique_key)])

    latex.__finish__()

    with open(MAIN_FOLDER + "latex_report/latex.tex", "wa") as f:
        f.write(latex.text.encode('ascii', 'ignore'))


def read_html_files():

    for index, file in enumerate(glob.glob(MAIN_FOLDER + "archive/*.html")):
        print 'File %d imported!' % index, file
        address = "file://" + file.replace(' ', '%20')
        read_citation_html(address)

    networkx.write_gml(network, MAIN_FOLDER + "paper_network.gml")
    with open(MAIN_FOLDER + "network.pkl", "wa") as f:
        pickle.dump(network, f)
    with open(MAIN_FOLDER + "paper_dict.pkl", "wa") as f:
        pickle.dump(paper_dict, f)



def get_statistics():
    with open(MAIN_FOLDER + "network.pkl", "r") as f:
        network = pickle.load(f)

    betweenness_dict = networkx.betweenness_centrality(network.to_undirected())
    sorted_betweenness = sorted(betweenness_dict.items(), key=operator.itemgetter(1), reverse=True)
    betweenness = [x[0] for x in sorted_betweenness[:5]]

    pagerank_dict = networkx.pagerank(network.to_undirected())
    sorted_pagerank = sorted(pagerank_dict.items(), key=operator.itemgetter(1), reverse=True)
    pagerank = [x[0] for x in sorted_pagerank[:5]]

    hits_dict = networkx.hits(network.to_undirected())
    sorted_hits = sorted(hits_dict[0].items(), key=operator.itemgetter(1), reverse=True)
    hits = [x[0] for x in sorted_hits[:5]]

    in_degree_dict = network.in_degree()
    sorted_in_degree = sorted(in_degree_dict.items(), key=operator.itemgetter(1), reverse=True)
    in_degree = [x[0] for x in sorted_in_degree[:5]]

    community_dict = []
    for k in xrange(20):
        community_dict += list(networkx.k_clique_communities(network.to_undirected(), 21 - k))
        if community_dict:
            break

    modules = []
    for index, community in enumerate(community_dict):
        modules.append([])
        for p in community:
            modules[index].append(p)

    statistics = {'in_degree': in_degree, 'betweenness': betweenness, 'hits': hits, 'pagerank': pagerank,
                  'modules': modules}

    with open(MAIN_FOLDER + "statistics.pkl", "wa") as f:
        pickle.dump(statistics, f)


def auto_explore():
    tmp_list = []
    for paper in paper_dict.values():
        tmp_list.append(paper)

    random.shuffle(tmp_list)

    for paper in tmp_list:
        if not paper.cites:
            webbrowser.open_new_tab(paper.citation_url)
            time.sleep(3)


if __name__ == "__main__":
    with open(MAIN_FOLDER + "paper_dict.pkl", "r") as f:
        paper_dict = pickle.load(f)

    with open(MAIN_FOLDER + "network.pkl", "r") as f:
        network = pickle.load(f)

    with open(MAIN_FOLDER + "statistics.pkl", "r") as f:
        statistics = pickle.load(f)

    # auto_explore()
    read_html_files()
    get_statistics()
    write_latex()
